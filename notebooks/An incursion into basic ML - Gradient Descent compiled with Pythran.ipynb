{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In https://realpython.com/numpy-tensorflow-performance/, the author compares the performance of different approaches of a basic ML kernel, gradient descent. \n",
    "\n",
    "Let's try to join the party :-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pythran\n",
    "%load_ext pythran.magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Setup\n",
    "========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original Numpy code is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools as it\n",
    "\n",
    "def np_descent(x, d, mu, N_epochs):\n",
    "    d = d.squeeze()\n",
    "    N = len(x)\n",
    "    f = 2 / N\n",
    "\n",
    "    y = np.zeros(N)\n",
    "    err = np.zeros(N)\n",
    "    w = np.zeros(2)\n",
    "    grad = np.empty(2)\n",
    "\n",
    "    for _ in it.repeat(None, N_epochs):\n",
    "        np.subtract(d, y, out=err)\n",
    "        grad[:] = f * np.sum(err), f * (err @ x)\n",
    "        w = w + mu * grad\n",
    "        y = w[0] + w[1] * x\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the experimental setup is the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(444)\n",
    "\n",
    "N = 10000\n",
    "sigma = 0.1\n",
    "noise = sigma * np.random.randn(N)\n",
    "x = np.linspace(0, 2, N)\n",
    "d = 3 + 2 * x + noise\n",
    "d.shape = (N, 1)\n",
    "\n",
    "mu = 0.001\n",
    "N_epochs = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our base line is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281 ms ± 9.82 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np_descent(x, d, mu, N_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythran version\n",
    "========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the implicit contract with pythran is ‘add a comment and compile’, but in that case we made two changes:\n",
    "\n",
    "1. static ``squeeze`` because pythran does not support dynamic array dimensions\n",
    "2. remove the ``out`` parameter for ``np.subtract`` because it's not supported yet by pythran (but it could in the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pythran\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "\n",
    "#pythran export pythran_descent(float64[], float64[,], float, int)\n",
    "def pythran_descent(x, d, mu, N_epochs):\n",
    "    assert d.shape[1] == 1, \"pythran does not support squeeze\"\n",
    "    d = d.reshape(d.shape[0])\n",
    "    N = len(x)\n",
    "    f = 2 / N\n",
    "\n",
    "    y = np.zeros(N)\n",
    "    err = np.zeros(N)\n",
    "    w = np.zeros(2)\n",
    "    grad = np.empty(2)\n",
    "\n",
    "    for _ in it.repeat(None, N_epochs):\n",
    "        err[:] = d - y\n",
    "        grad[:] = f * np.sum(err), f * (err @ x)\n",
    "        w = w + mu * grad\n",
    "        y = w[0] + w[1] * x\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it compiles fine, let's run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268 ms ± 5.05 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pythran_descent(x, d, mu, N_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's slightly faster, but not by much. The numpy code is actually pretty good already, and a good chunk of the time is spent in the scalar product; there is not much to gain here as both numpy and pythran fallback to blas.\n",
    "\n",
    "SIMD Instructions to the rescue\n",
    "------------------------------------\n",
    "\n",
    "Pythran supports generation of SIMD instructions, through the great Boost.SIMD library. Let's update compile flags and try again. The ``-march=native`` tells the underlying compiler (here, GCC 7.3.0) to generate code specific to my processor's architecture, thus enabling AVX instructions \\o/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pythran -DUSE_BOOST_SIMD -march=native\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "\n",
    "#pythran export pythran_descent_simd(float64[], float64[,], float, int)\n",
    "def pythran_descent_simd(x, d, mu, N_epochs):\n",
    "    assert d.shape[1] == 1, \"pythran does not support squeeze\"\n",
    "    d = d.reshape(d.shape[0])\n",
    "    N = len(x)\n",
    "    f = 2 / N\n",
    "\n",
    "    y = np.zeros(N)\n",
    "    err = np.zeros(N)\n",
    "    w = np.zeros(2)\n",
    "    grad = np.empty(2)\n",
    "\n",
    "    for _ in it.repeat(None, N_epochs):\n",
    "        err[:] = d - y\n",
    "        grad[:] = f * np.sum(err), f * (err @ x)\n",
    "        w = w + mu * grad\n",
    "        y = w[0] + w[1] * x\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 ms ± 298 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pythran_descent_simd(x, d, mu, N_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now *that* is fast \\o/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The long story\n",
    "========\n",
    "\n",
    "When I first tried to port the kernel, there was two limitations in Pythran. They are now merged into master but not in current release (0.8.5).\n",
    "\n",
    "1. There was no support for ``itertools.repeat``. Pythran already supports a bunch of the ``itertools`` interface, so even if it's a bit overkill in that context, i added the support and the tests for that call.\n",
    "\n",
    "2. Poor ``@`` performance. In the case of the scalar product of two arrays, openblas is much faster than the trivial non-vectorized implementation, so I specialized the pythran implementation of dot to fallback to the blas call when both parameters are arrays. In the more generic case, merging the operation is still a better approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pythran -DUSE_BOOST_SIMD -march=native\n",
    "#pythran export dottest0(float[], float[])\n",
    "def dottest0(x, y):\n",
    "    from numpy import array\n",
    "    tmp = x + y\n",
    "    return x @ tmp, tmp\n",
    "\n",
    "#pythran export dottest1(float[], float[])\n",
    "def dottest1(x, y):\n",
    "    from numpy import array\n",
    "    tmp = x + y\n",
    "    return x @ tmp, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = y = np.ones(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.74 ms ± 12.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dottest0(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631 µs ± 33.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dottest1(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened? In ``dottest0``, ``tmp`` is used twice so a temporary array is created, and the ``@`` operator fallsback to blas implementation, as it is specialized in that case. For ``dottest1``, ``tmp`` is used once, so it is evaluated lazily and the ``@`` operator now has an array and a lazy expression as parameter: it computes this expression in a single (vectorized) loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Words\n",
    "======\n",
    "\n",
    "So here are the final timings from my little experiment. It's nice to get some speedups from high level code, and I should probably be able to improve the generated code in the future!\n",
    "\n",
    "|Engine      | Execution Time (s)\n",
    "-------------|--------------\n",
    "|Numpy       | 0.281\n",
    "|Pythran     | 0.268\n",
    "|Pythran+SIMD| 0.114"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
